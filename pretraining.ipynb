{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1SiWHewIa4raRD927MRoY32DfObSB9LE7",
      "authorship_tag": "ABX9TyNY/gAf0l5HwfOOV6of/CSu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karaxstone7/PS-10/blob/main/pretraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1soBp3v-b2Vt",
        "outputId": "1034fb73-ae89-464f-f316-41e67fa0896c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_FuOBGPugka",
        "outputId": "829d0c85-1c25-45b1-f904-fa73622962d8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install gdown\n",
        "FILE_ID = \"1SQkVnzePJssdMiRzEI1xPpaeNxo0guOy\"  # e.g. \"1abcDEFghIJ...\"\n",
        "!mkdir -p \"/content/drive/My Drive/datasets/mock\"\n",
        "!gdown --id $FILE_ID -O \"/content/drive/My Drive/datasets/mock/L1C1.zip\"\n"
      ],
      "metadata": {
        "id": "0Z2dd_XNsiUt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d9aa46f-f5b6-416e-c4cb-cd81312da5cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1SQkVnzePJssdMiRzEI1xPpaeNxo0guOy\n",
            "From (redirected): https://drive.google.com/uc?id=1SQkVnzePJssdMiRzEI1xPpaeNxo0guOy&confirm=t&uuid=2dd0a91f-b586-488f-8035-cfa0959af22b\n",
            "To: /content/drive/My Drive/datasets/mock/L1C1.zip\n",
            "100% 810M/810M [00:12<00:00, 66.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install gdown\n",
        "FILE_ID = \"1TxLxTmjPS11vMJq9hRs3wx4QZiUW60Fa\"  # e.g. \"1abcDEFghIJ...\"\n",
        "!mkdir -p \"/content/drive/My Drive/datasets/mock\"\n",
        "!gdown --id $FILE_ID -O \"/content/drive/My Drive/datasets/mock/L2A2.zip\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2USoGTAe0VrU",
        "outputId": "17fac8c9-abee-4feb-e536-a507e236a57d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1TxLxTmjPS11vMJq9hRs3wx4QZiUW60Fa\n",
            "From (redirected): https://drive.google.com/uc?id=1TxLxTmjPS11vMJq9hRs3wx4QZiUW60Fa&confirm=t&uuid=f0e416d2-644d-43db-a9a0-93c93dc58171\n",
            "To: /content/drive/My Drive/datasets/mock/L2A2.zip\n",
            "100% 1.20G/1.20G [00:18<00:00, 64.8MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil, pathlib, zipfile, glob\n",
        "\n",
        "ROOT_DRIVE = pathlib.Path(\"/content/drive/My Drive/datasets\")\n",
        "RAW_DIR    = ROOT_DRIVE / \"S2Looking_raw\"\n",
        "OUT_DIR    = ROOT_DRIVE / \"S2Looking\"\n",
        "\n",
        "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 1) Unzip anything in RAW_DIR (skip if already unzipped)\n",
        "for z in RAW_DIR.glob(\"*.zip\"):\n",
        "    with zipfile.ZipFile(z, 'r') as f:\n",
        "        f.extractall(RAW_DIR)\n"
      ],
      "metadata": {
        "id": "IS8w_AfKgol5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting the dataset into train,val and test\n"
      ],
      "metadata": {
        "id": "dKvUg_AKRkHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, shutil\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "\n",
        "SRC_ROOT = Path(\"/content/drive/MyDrive/datasets/S2Looking_raw/S2Looking\")\n",
        "DST_ROOT = Path(\"/content/drive/MyDrive/datasets/S2Looking_MobileCDNet\")\n",
        "\n",
        "(A := DST_ROOT/\"A\").mkdir(parents=True, exist_ok=True)\n",
        "(B := DST_ROOT/\"B\").mkdir(parents=True, exist_ok=True)\n",
        "(L := DST_ROOT/\"label\").mkdir(parents=True, exist_ok=True)\n",
        "(LIST := DST_ROOT/\"list\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "splits = [\"train\",\"val\",\"test\"]\n",
        "ACCEPT_EXTS = {\".png\",\".jpg\",\".jpeg\",\".tif\",\".tiff\",\".bmp\"}\n",
        "\n",
        "def rcollect(folder, exts=ACCEPT_EXTS):\n",
        "    files = []\n",
        "    for ext in exts:\n",
        "        files += list(folder.rglob(f\"*{ext}\"))\n",
        "    return sorted(files)\n",
        "\n",
        "def looks_like_images_dir(d: Path, min_files=50):\n",
        "    if not d or not d.exists() or not d.is_dir():\n",
        "        return False\n",
        "    files = rcollect(d)\n",
        "    return len(files) >= min_files\n",
        "\n",
        "def find_modality_dir_for_split(root: Path, split: str, modality_candidates=(\"img1\",\"image1\",\"A\",\"t1\",\"img2\",\"image2\",\"B\",\"t2\")):\n",
        "    \"\"\"\n",
        "    Returns dict {'img1': Path, 'img2': Path} for the split if found, else raises.\n",
        "    Tries:\n",
        "      Case A: root/split/{img1,img2,...}\n",
        "      Case B: root/{img1,img2,...}/split\n",
        "      Fallback: global search for dirs containing split in path and having many images.\n",
        "    \"\"\"\n",
        "    # Case A\n",
        "    caseA = {}\n",
        "    baseA = root/split\n",
        "    for mod in modality_candidates:\n",
        "        d = baseA/mod\n",
        "        if looks_like_images_dir(d):\n",
        "            caseA[mod] = d\n",
        "    # Assign best matches to roles\n",
        "    def choose(dmap, want=(\"img1\",\"image1\",\"A\",\"t1\"), alt=(\"img2\",\"image2\",\"B\",\"t2\")):\n",
        "        d1 = next((dmap[k] for k in want if k in dmap), None)\n",
        "        d2 = next((dmap[k] for k in alt  if k in dmap), None)\n",
        "        return d1, d2\n",
        "\n",
        "    d1, d2 = choose(caseA)\n",
        "    if d1 and d2:\n",
        "        return {\"img1\": d1, \"img2\": d2, \"layout\":\"split-first\"}\n",
        "\n",
        "    # Case B\n",
        "    caseB = {}\n",
        "    for mod in modality_candidates:\n",
        "        d = root/mod/split\n",
        "        if looks_like_images_dir(d):\n",
        "            caseB[mod] = d\n",
        "    d1, d2 = choose(caseB)\n",
        "    if d1 and d2:\n",
        "        return {\"img1\": d1, \"img2\": d2, \"layout\":\"modality-first\"}\n",
        "\n",
        "    # Fallback: global search (one level deeper) for anything with split in its path and many images\n",
        "    candidates = []\n",
        "    for p in root.rglob(\"*\"):\n",
        "        try:\n",
        "            if p.is_dir() and split in str(p):\n",
        "                if looks_like_images_dir(p, min_files=10):\n",
        "                    candidates.append(p)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # Heuristic: pick two distinct parents that look like img1/img2 sides\n",
        "    # Prefer paths containing keywords\n",
        "    def score_dir(p: Path):\n",
        "        s = str(p).lower()\n",
        "        score = 0\n",
        "        if \"img1\" in s or \"image1\" in s or re.search(r\"(^|/|_)(a|t1)(/|_|$)\", s):\n",
        "            score += 2\n",
        "        if \"img2\" in s or \"image2\" in s or re.search(r\"(^|/|_)(b|t2)(/|_|$)\", s):\n",
        "            score += 1\n",
        "        # more files = slightly higher\n",
        "        score += min(len(rcollect(p))//100, 3)\n",
        "        return score\n",
        "\n",
        "    # Split into two groups by keyword\n",
        "    group1 = [p for p in candidates if re.search(r\"img1|image1|(^|/|_)(a|t1)(/|_|$)\", str(p).lower())]\n",
        "    group2 = [p for p in candidates if re.search(r\"img2|image2|(^|/|_)(b|t2)(/|_|$)\", str(p).lower())]\n",
        "\n",
        "    d1 = max(group1, key=score_dir) if group1 else None\n",
        "    d2 = max(group2, key=score_dir) if group2 else None\n",
        "    if d1 and d2:\n",
        "        return {\"img1\": d1, \"img2\": d2, \"layout\":\"fallback-search\"}\n",
        "\n",
        "    raise RuntimeError(f\"[{split}] Could not locate img1/img2 dirs anywhere under {root}.\")\n",
        "\n",
        "def norm_key(p: Path):\n",
        "    s = p.stem.lower()\n",
        "    s = re.sub(r\"^((a|b|t1|t2|img1|img2|image1|image2)[-_]+)+\",\"\",s)\n",
        "    s = re.sub(r\"([-_](a|b|t1|t2|1|2|img1|img2|image1|image2|label|gt|change))+$\",\"\",s)\n",
        "    return s\n",
        "\n",
        "def prefer_png(paths):\n",
        "    if not paths: return None\n",
        "    png = [p for p in paths if p.suffix.lower()==\".png\"]\n",
        "    return png[0] if png else paths[0]\n",
        "\n",
        "def to_png(src: Path, dst: Path, is_label=False):\n",
        "    if src.suffix.lower()==\".png\":\n",
        "        shutil.copy2(src, dst)\n",
        "    else:\n",
        "        img = Image.open(src)\n",
        "        img = img.convert(\"L\") if is_label and img.mode!=\"L\" else (img.convert(\"RGB\") if not is_label else img)\n",
        "        img.save(dst)\n",
        "\n",
        "summary = {}\n",
        "\n",
        "for sp in splits:\n",
        "    base = SRC_ROOT/sp\n",
        "    # labels are already here for train/val (as you showed)\n",
        "    label_dir = base/\"label\"\n",
        "    if sp != \"test\" and not label_dir.exists():\n",
        "        raise RuntimeError(f\"[{sp}] Expected {label_dir} to exist for labels.\")\n",
        "\n",
        "    # find img1/img2 for this split\n",
        "    mods = find_modality_dir_for_split(SRC_ROOT, sp)\n",
        "    d1, d2 = mods[\"img1\"], mods[\"img2\"]\n",
        "    print(f\"[{sp}] using {mods['layout']}  img1={d1}  img2={d2}\")\n",
        "\n",
        "    # index files by normalized key\n",
        "    files1 = rcollect(d1); files2 = rcollect(d2)\n",
        "    idx1, idx2 = {}, {}\n",
        "    for p in files1: idx1.setdefault(norm_key(p), []).append(p)\n",
        "    for p in files2: idx2.setdefault(norm_key(p), []).append(p)\n",
        "\n",
        "    idxL = {}\n",
        "    if sp != \"test\":\n",
        "        filesL = rcollect(label_dir, exts={\".png\"})  # your labels are .png\n",
        "        for p in filesL: idxL.setdefault(norm_key(p), []).append(p)\n",
        "\n",
        "    if sp == \"test\":\n",
        "        common = sorted(set(idx1) & set(idx2))\n",
        "    else:\n",
        "        common = sorted(set(idx1) & set(idx2) & set(idxL))\n",
        "\n",
        "    if not common:\n",
        "        raise RuntimeError(f\"[{sp}] No matching basenames after normalization.\")\n",
        "\n",
        "    written = 0\n",
        "    with open(DST_ROOT/\"list\"/f\"{sp}.txt\",\"w\") as lf:\n",
        "        for k in common:\n",
        "            p1 = prefer_png(idx1[k]); p2 = prefer_png(idx2[k])\n",
        "            outname = f\"{k}.png\"\n",
        "            to_png(p1, (A/outname))\n",
        "            to_png(p2, (B/outname))\n",
        "            if sp != \"test\":\n",
        "                pL = prefer_png(idxL[k])\n",
        "                to_png(pL, (L/outname), is_label=True)\n",
        "            lf.write(outname+\"\\n\")\n",
        "            written += 1\n",
        "\n",
        "    summary[sp] = {\"pairs\": written, \"candidates\": len(common)}\n",
        "\n",
        "print(\"\\nDone. Target:\", DST_ROOT)\n",
        "print(\"A:\", len(list((DST_ROOT/'A').glob('*.png'))),\n",
        "      \"B:\", len(list((DST_ROOT/'B').glob('*.png'))),\n",
        "      \"L:\", len(list((DST_ROOT/'label').glob('*.png'))))\n",
        "for sp in splits:\n",
        "    p = DST_ROOT/'list'/f\"{sp}.txt\"\n",
        "    print(sp, \"list lines:\", (sum(1 for _ in open(p)) if p.exists() else 0))\n",
        "print(\"Summary:\", summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zo5P-6XPOGhz",
        "outputId": "bc77460c-68e4-43b6-b35b-8df9d97d87ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train] using fallback-search  img1=/content/drive/MyDrive/datasets/S2Looking_raw/S2Looking/train/Image1  img2=/content/drive/MyDrive/datasets/S2Looking_raw/S2Looking/train/Image2\n",
            "[val] using fallback-search  img1=/content/drive/MyDrive/datasets/S2Looking_raw/S2Looking/val/Image1  img2=/content/drive/MyDrive/datasets/S2Looking_raw/S2Looking/val/Image2\n",
            "[test] using fallback-search  img1=/content/drive/MyDrive/datasets/S2Looking_raw/S2Looking/test/Image1  img2=/content/drive/MyDrive/datasets/S2Looking_raw/S2Looking/test/Image2\n",
            "\n",
            "Done. Target: /content/drive/MyDrive/datasets/S2Looking_MobileCDNet\n",
            "A: 5000 B: 5000 L: 4000\n",
            "train list lines: 3500\n",
            "val list lines: 500\n",
            "test list lines: 1000\n",
            "Summary: {'train': {'pairs': 3500, 'candidates': 3500}, 'val': {'pairs': 500, 'candidates': 500}, 'test': {'pairs': 1000, 'candidates': 1000}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (Optional) Clean + pin versions known to work with this repo\n",
        "!pip uninstall -y opencv-contrib-python thinc || true\n",
        "!pip install -q numpy==1.26.4 tqdm==4.67.1 albumentations==1.4.10 opencv-python==4.10.0.84\n",
        "\n",
        "# The repo suggests torch 1.8.x; Colab ships 2.x which may also work,\n",
        "# but if you hit a Torch/TV mismatch, install the pair below:\n",
        "# !pip install -q torch==1.8.1+cu111 torchvision==0.9.1+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pcPiUyWOqCj",
        "outputId": "79ac4ab2-061b-49d7-f0b4-92db166253d9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: opencv-contrib-python 4.12.0.88\n",
            "Uninstalling opencv-contrib-python-4.12.0.88:\n",
            "  Successfully uninstalled opencv-contrib-python-4.12.0.88\n",
            "Found existing installation: thinc 8.3.6\n",
            "Uninstalling thinc-8.3.6:\n",
            "  Successfully uninstalled thinc-8.3.6\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m115.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.9/161.9 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy 3.8.7 requires thinc<8.4.0,>=8.3.4, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/tawneydaylily/Mobile-CDNet.git\n",
        "%cd Mobile-CDNet\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLYp2sSZd25K",
        "outputId": "a276e142-417b-4b8a-fc6d-05d2c5b9ab32"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Mobile-CDNet'...\n",
            "remote: Enumerating objects: 100, done.\u001b[K\n",
            "remote: Counting objects: 100% (100/100), done.\u001b[K\n",
            "remote: Compressing objects: 100% (97/97), done.\u001b[K\n",
            "remote: Total 100 (delta 12), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (100/100), 10.51 MiB | 14.62 MiB/s, done.\n",
            "Resolving deltas: 100% (12/12), done.\n",
            "/content/Mobile-CDNet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Ensure we are at the repo root\n",
        "%cd /content/Mobile-CDNet\n",
        "!pwd\n",
        "!ls -la\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53UG5U9ndBpm",
        "outputId": "0bae3e8b-5c6e-46d5-a089-e37a0c528279"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Mobile-CDNet\n",
            "/content/Mobile-CDNet\n",
            "total 48\n",
            "drwxr-xr-x 6 root root 4096 Oct  9 15:15 .\n",
            "drwxr-xr-x 1 root root 4096 Oct  9 15:15 ..\n",
            "drwxr-xr-x 2 root root 4096 Oct  9 15:15 chap4\n",
            "-rw-r--r-- 1 root root 1598 Oct  9 15:15 dataset.py\n",
            "drwxr-xr-x 8 root root 4096 Oct  9 15:15 .git\n",
            "-rw-r--r-- 1 root root 3990 Oct  9 15:15 metric_tool.py\n",
            "drwxr-xr-x 5 root root 4096 Oct  9 15:15 models\n",
            "-rw-r--r-- 1 root root 2257 Oct  9 15:15 README.md\n",
            "drwxr-xr-x 2 root root 4096 Oct  9 15:15 tools\n",
            "-rw-r--r-- 1 root root 6705 Oct  9 15:15 Transforms.py\n",
            "-rw-r--r-- 1 root root  431 Oct  9 15:15 utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) See if a \"models\" folder exists and whether it has __init__.py\n",
        "!find . -maxdepth 2 -type d -iname \"models\" -print\n",
        "!ls -la ./models || true\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsDBXoW9d8F8",
        "outputId": "53d27158-534b-4254-a53b-6bdc77371fce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./models\n",
            "./Mobile-CDNet/models\n",
            "total 116\n",
            "drwxr-xr-x 5 root root  4096 Oct  9 10:11 .\n",
            "drwxr-xr-x 7 root root  4096 Oct  9 10:17 ..\n",
            "drwxr-xr-x 3 root root  4096 Oct  9 10:11 backbones\n",
            "-rw-r--r-- 1 root root  5073 Oct  9 10:11 bam.py\n",
            "-rw-r--r-- 1 root root 12618 Oct  9 10:11 bit.py\n",
            "-rw-r--r-- 1 root root  3235 Oct  9 10:11 _blocks.py\n",
            "-rw-r--r-- 1 root root    21 Oct  9 10:11 __init__.py\n",
            "-rw-r--r-- 1 root root  4188 Oct  9 10:11 MobileNetV2.py\n",
            "-rw-r--r-- 1 root root 18256 Oct  9 10:11 model91.79.py\n",
            "-rw-r--r-- 1 root root  3841 Oct  9 10:11 model.py\n",
            "-rw-r--r-- 1 root root  5270 Oct  9 10:11 nlfpn.py\n",
            "-rw-r--r-- 1 root root  6646 Oct  9 10:11 pam.py\n",
            "drwxr-xr-x 2 root root  4096 Oct  9 10:11 __pycache__\n",
            "-rw-r--r-- 1 root root  9441 Oct  9 10:11 resnet.py\n",
            "drwxr-xr-x 3 root root  4096 Oct  9 10:11 sync_batchnorm\n",
            "-rw-r--r-- 1 root root   512 Oct  9 10:11 _utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) Make sure Python treats folders as packages\n",
        "# (many research repos forget these files)\n",
        "!touch ./models/__init__.py\n",
        "!touch ./tools/__init__.py\n"
      ],
      "metadata": {
        "id": "-xjdSAyXej9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) Double-check Python sees the repo root first on sys.path\n",
        "\n",
        "import os, sys\n",
        "print(\"CWD:\", os.getcwd())\n",
        "print(\"Has ./models?\", os.path.isdir(\"models\"))\n",
        "print(\"models/__init__.py exists?\", os.path.isfile(\"models/__init__.py\"))\n",
        "print(\"tools/__init__.py exists?\", os.path.isfile(\"tools/__init__.py\"))\n",
        "print(\"First 3 sys.path entries:\", sys.path[:3])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWIgRjvselpo",
        "outputId": "3754c06a-05f0-44b5-ca03-2237a5441d64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CWD: /content/Mobile-CDNet\n",
            "Has ./models? True\n",
            "models/__init__.py exists? True\n",
            "tools/__init__.py exists? True\n",
            "First 3 sys.path entries: ['/content', '/env/python', '/usr/lib/python312.zip']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check that the dataset is laid out correctly\n",
        "!ls -la /content/drive/MyDrive/datasets/S2Looking_MobileCDNet\n",
        "!ls -la /content/drive/MyDrive/datasets/S2Looking_MobileCDNet/list\n",
        "\n",
        "# Peek at the first few lines in each split list\n",
        "!head -n 5 /content/drive/MyDrive/datasets/S2Looking_MobileCDNet/list/train.txt\n",
        "!head -n 5 /content/drive/MyDrive/datasets/S2Looking_MobileCDNet/list/val.txt\n",
        "!head -n 5 /content/drive/MyDrive/datasets/S2Looking_MobileCDNet/list/test.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kk4yx9u6fhOq",
        "outputId": "be8e8892-e54c-4a83-cdda-03150bff063e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 16\n",
            "drwx------ 2 root root 4096 Oct  9 09:06 A\n",
            "drwx------ 2 root root 4096 Oct  9 09:06 B\n",
            "drwx------ 2 root root 4096 Oct  9 09:06 label\n",
            "drwx------ 2 root root 4096 Oct  9 09:06 list\n",
            "total 44\n",
            "-rw------- 1 root root  8785 Oct  9 10:03 test.txt\n",
            "-rw------- 1 root root 30704 Oct  9 09:52 train.txt\n",
            "-rw------- 1 root root  4404 Oct  9 09:52 val.txt\n",
            "1.png\n",
            "10.png\n",
            "100.png\n",
            "1000.png\n",
            "1002.png\n",
            "1001.png\n",
            "1016.png\n",
            "1020.png\n",
            "1045.png\n",
            "1054.png\n",
            "1008.png\n",
            "1011.png\n",
            "1021.png\n",
            "1023.png\n",
            "1029.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Mobile-CDNet\n",
        "\n",
        "# Make a backup\n",
        "!cp tools/train.py tools/train.py.bak\n",
        "\n",
        "# Patch: allow absolute/relative paths if they exist; keep legacy shortcuts too\n",
        "\n",
        "from pathlib import Path\n",
        "p = Path(\"tools/train.py\")\n",
        "src = p.read_text()\n",
        "\n",
        "old = \"\"\"if args.file_root == 'LEVIR':\n",
        "        args.file_root = 'H:\\\\\\\\penghaifeng\\\\\\\\LEVIR-CD'\n",
        "    elif args.file_root == 'BCDD':\n",
        "        args.file_root = 'H:\\\\\\\\penghaifeng\\\\\\\\BCDD'\n",
        "    elif args.file_root == 'SYSU':\n",
        "        args.file_root = 'H:\\\\\\\\penghaifeng\\\\\\\\SYSU-CD'\n",
        "    elif args.file_root == 'CDD':\n",
        "        args.file_root = '/home/guan/Documents/Datasets/ChangeDetection/CDD'\n",
        "    elif args.file_root == 'quick_start':\n",
        "        args.file_root = './samples'\n",
        "    else:\n",
        "        raise TypeError('%s has not defined' % args.file_root)\"\"\"\n",
        "new = \"\"\"# Accept known aliases, otherwise accept any existing path\n",
        "    if args.file_root == 'LEVIR':\n",
        "        args.file_root = 'H:\\\\\\\\penghaifeng\\\\\\\\LEVIR-CD'\n",
        "    elif args.file_root == 'BCDD':\n",
        "        args.file_root = 'H:\\\\\\\\penghaifeng\\\\\\\\BCDD'\n",
        "    elif args.file_root == 'SYSU':\n",
        "        args.file_root = 'H:\\\\\\\\penghaifeng\\\\\\\\SYSU-CD'\n",
        "    elif args.file_root == 'CDD':\n",
        "        args.file_root = '/home/guan/Documents/Datasets/ChangeDetection/CDD'\n",
        "    elif args.file_root == 'quick_start':\n",
        "        args.file_root = './samples'\n",
        "    else:\n",
        "        # If it's an existing directory (e.g., your custom dataset), use it directly\n",
        "        import os\n",
        "        if not os.path.isdir(args.file_root):\n",
        "            raise TypeError('%s has not defined' % args.file_root)\"\"\"\n",
        "\n",
        "p.write_text(src.replace(old, new))\n",
        "print(\"Patched tools/train.py\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlaPuRrhjY68",
        "outputId": "63705c53-dc42-4508-b01f-7139bab9cf9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Mobile-CDNet\n",
            "Patched tools/train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cat > dataset.py << 'PY'\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch.utils.data as data\n",
        "\n",
        "class Dataset(data.Dataset):\n",
        "    \"\"\"\n",
        "    Mobile-CD style loader:\n",
        "    file_root/\n",
        "      A/      B/      label/\n",
        "      list/train.txt  list/val.txt  list/test.txt\n",
        "    Each list contains bare filenames like:  0001.png\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset, file_root='data/', transform=None):\n",
        "        self.split = dataset  # \"train\" | \"val\" | \"test\"\n",
        "        self.file_root = file_root\n",
        "        self.transform = transform\n",
        "\n",
        "        # Prefer file_root/list/<split>.txt; fallback to file_root/<split>.txt\n",
        "        cand_lists = [\n",
        "            os.path.join(file_root, \"list\", f\"{dataset}.txt\"),\n",
        "            os.path.join(file_root, f\"{dataset}.txt\"),\n",
        "            # last resort (old style the repo expected, but not your case)\n",
        "            os.path.join(file_root, dataset, \"list\", f\"{dataset}.txt\"),\n",
        "        ]\n",
        "        list_path = next((p for p in cand_lists if os.path.isfile(p)), None)\n",
        "        if list_path is None:\n",
        "            raise FileNotFoundError(f\"List file not found. Tried: {cand_lists}\")\n",
        "\n",
        "        with open(list_path, \"r\") as f:\n",
        "            names = [ln.strip() for ln in f if ln.strip()]\n",
        "\n",
        "        A_dir = os.path.join(file_root, \"A\")\n",
        "        B_dir = os.path.join(file_root, \"B\")\n",
        "        L_dir = os.path.join(file_root, \"label\")\n",
        "\n",
        "        pre_images, post_images, gts = [], [], []\n",
        "        for n in names:\n",
        "            pa = os.path.join(A_dir, n)\n",
        "            pb = os.path.join(B_dir, n)\n",
        "            pl = os.path.join(L_dir, n)\n",
        "            # Require GT for all splits (your set has labels for train/val; keep test too if present)\n",
        "            if os.path.isfile(pa) and os.path.isfile(pb) and os.path.isfile(pl):\n",
        "                pre_images.append(pa)\n",
        "                post_images.append(pb)\n",
        "                gts.append(pl)\n",
        "\n",
        "        if len(pre_images) == 0:\n",
        "            raise RuntimeError(f\"No valid samples for split='{dataset}' under {file_root}\")\n",
        "\n",
        "        self.pre_images = pre_images\n",
        "        self.post_images = post_images\n",
        "        self.gts = gts\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pre_images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        pre_path = self.pre_images[idx]\n",
        "        post_path = self.post_images[idx]\n",
        "        gt_path = self.gts[idx]\n",
        "\n",
        "        pre = cv2.imread(pre_path, cv2.IMREAD_COLOR)   # BGR\n",
        "        post = cv2.imread(post_path, cv2.IMREAD_COLOR)\n",
        "        if pre is None or post is None:\n",
        "            raise RuntimeError(f\"Failed to read: {pre_path} or {post_path}\")\n",
        "\n",
        "        pre = cv2.cvtColor(pre, cv2.COLOR_BGR2RGB)\n",
        "        post = cv2.cvtColor(post, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        lab = cv2.imread(gt_path, cv2.IMREAD_GRAYSCALE)\n",
        "        if lab is None:\n",
        "            raise RuntimeError(f\"Failed to read label: {gt_path}\")\n",
        "        # binarize: >0 → 1.0, keep shape (H, W, 1)\n",
        "        lab = (lab > 0).astype(np.float32)[..., None]\n",
        "\n",
        "        # concat 6 channels, scale to [0,1] for Normalize(mean/std in train.py)\n",
        "        img6 = np.concatenate([pre, post], axis=2).astype(np.float32) / 255.0\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img6, lab = self.transform(img6, lab)\n",
        "\n",
        "        return img6, lab\n",
        "\n",
        "    def get_img_info(self, idx):\n",
        "        img = cv2.imread(self.pre_images[idx])\n",
        "        return {\"height\": img.shape[0], \"width\": img.shape[1]}\n",
        "PY\n"
      ],
      "metadata": {
        "id": "DFHbqn6TlLbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Step 1 — paths\n",
        "repo = Path(\"/content/Mobile-CDNet\")\n",
        "train_py = repo / \"tools/train.py\"\n",
        "backup = repo / \"tools/train.py.bak\"\n",
        "\n",
        "# Step 2 — back up safely\n",
        "backup.write_text(train_py.read_text())\n",
        "\n",
        "# Step 3 — read and patch\n",
        "text = train_py.read_text()\n",
        "\n",
        "# a) Lower num_workers default (optional)\n",
        "text = text.replace(\n",
        "    \"parser.add_argument('--num_workers', type=int, default=4\",\n",
        "    \"parser.add_argument('--num_workers', type=int, default=2\"\n",
        ")\n",
        "\n",
        "# b) Wrap test loader in try/except to skip missing test split\n",
        "import re\n",
        "pattern_testblock = re.compile(\n",
        "    r'    test_data = myDataLoader\\.Dataset\\(\"test\".*?pin_memory=False\\)', re.S\n",
        ")\n",
        "replacement_block = \"\"\"    # Optional test set (skip if unavailable)\n",
        "    has_test = True\n",
        "    try:\n",
        "        test_data = myDataLoader.Dataset(\"test\", file_root=args.file_root, transform=valDataset)\n",
        "        testLoader = torch.utils.data.DataLoader(\n",
        "            test_data, shuffle=False,\n",
        "            batch_size=args.batch_size, num_workers=args.num_workers, pin_memory=False)\n",
        "    except Exception as e:\n",
        "        print(\"[Info] Test split unavailable or unlabeled — skipping test phase.\\\\nReason:\", e)\n",
        "        has_test = False\"\"\"\n",
        "text = pattern_testblock.sub(replacement_block, text)\n",
        "\n",
        "# c) Guard the final test evaluation\n",
        "pattern_final = re.compile(\n",
        "    r'    loss_test, score_test = val\\(args, testLoader, model, 0\\).*?logger\\.flush\\(\\)', re.S\n",
        ")\n",
        "replacement_final = \"\"\"    if has_test:\n",
        "        loss_test, score_test = val(args, testLoader, model, 0)\n",
        "        print(\"\\\\nTest :\\\\t Kappa (te) = %.4f\\\\t IoU (te) = %.4f\\\\t F1 (te) = %.4f\\\\t R (te) = %.4f\\\\t P (te) = %.4f\" %\n",
        "              (score_test['Kappa'], score_test['IoU'], score_test['F1'], score_test['recall'], score_test['precision']))\n",
        "        logger.write(\"\\\\n%s\\\\t\\\\t%.4f\\\\t\\\\t%.4f\\\\t\\\\t%.4f\\\\t\\\\t%.4f\\\\t\\\\t%.4f\" % (\n",
        "            'Test', score_test['Kappa'], score_test['IoU'], score_test['F1'],\n",
        "            score_test['recall'], score_test['precision']))\n",
        "        logger.flush()\"\"\"\n",
        "text = pattern_final.sub(replacement_final, text)\n",
        "\n",
        "# Step 4 — save\n",
        "train_py.write_text(text)\n",
        "print(\"✅ Patched tools/train.py successfully. Backup saved at:\", backup)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lP1I0IT0mat2",
        "outputId": "8a977f14-a8b9-45a5-d96c-95d40ebf7afd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Patched tools/train.py successfully. Backup saved at: /content/Mobile-CDNet/tools/train.py.bak\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "tf = Path(\"/content/Mobile-CDNet/Transforms.py\")\n",
        "src = tf.read_text()\n",
        "\n",
        "# 1) Fix the exact offending line (and any similar uses)\n",
        "src = src.replace(\"dtype=np.int)\", \"dtype=np.int64)\")\n",
        "src = src.replace(\"dtype = np.int)\", \"dtype=np.int64)\")\n",
        "\n",
        "# 2) (Optional, future-proof) fix other deprecated aliases if they exist\n",
        "src = src.replace(\"np.float)\", \"np.float64)\")\n",
        "src = src.replace(\"np.bool)\", \"np.bool_)\")  # or use .astype(bool) if it's a cast\n",
        "\n",
        "tf.write_text(src)\n",
        "print(\"Transforms.py patched.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLhGo7xlnQd5",
        "outputId": "2a3c817c-5b2b-42fa-864c-d99a472910ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transforms.py patched.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import shutil, json\n",
        "\n",
        "# --- paths ---\n",
        "DST_ROOT = Path(\"/content/drive/MyDrive/datasets/S2Looking_MobileCDNet\")  # has A, B, label, list\n",
        "SRC_ROOT = Path(\"/content/drive/MyDrive/datasets/S2Looking_raw/S2Looking\")# has train/val/test with img1,img2,label...\n",
        "\n",
        "assert DST_ROOT.exists(), \"DST_ROOT not found\"\n",
        "assert SRC_ROOT.exists(), \"SRC_ROOT not found\"\n",
        "\n",
        "# --- gather existing files ---\n",
        "A = {p.name for p in (DST_ROOT/\"A\").glob(\"*.png\")}\n",
        "B = {p.name for p in (DST_ROOT/\"B\").glob(\"*.png\")}\n",
        "L = {p.name for p in (DST_ROOT/\"label\").glob(\"*.png\")}\n",
        "\n",
        "# load current split lists (if present)\n",
        "split_files = {}\n",
        "split_names = {}\n",
        "for sp in [\"train\",\"val\",\"test\"]:\n",
        "    f = DST_ROOT/\"list\"/f\"{sp}.txt\"\n",
        "    if f.exists():\n",
        "        split_files[sp] = f\n",
        "        split_names[sp] = [ln.strip() for ln in f.read_text().splitlines() if ln.strip()]\n",
        "    else:\n",
        "        split_files[sp] = f\n",
        "        split_names[sp] = []\n",
        "\n",
        "# --- find orphans (in A & B but missing in label) ---\n",
        "orphans_ab = sorted((A & B) - L)\n",
        "\n",
        "# helper: try to find a label in raw for a given filename and (optionally) split\n",
        "def find_label_in_raw(name, prefer_split=None):\n",
        "    # 1) try preferred split first\n",
        "    if prefer_split:\n",
        "        cand = SRC_ROOT/prefer_split/\"label\"/name\n",
        "        if cand.exists():\n",
        "            return cand\n",
        "    # 2) try any split\n",
        "    for sp in [\"train\",\"val\",\"test\"]:\n",
        "        cand = SRC_ROOT/sp/\"label\"/name\n",
        "        if cand.exists():\n",
        "            return cand\n",
        "    return None\n",
        "\n",
        "# --- try to recover missing labels from raw ---\n",
        "recovered = []\n",
        "not_found = []\n",
        "for n in orphans_ab:\n",
        "    # infer preferred split from existing lists\n",
        "    prefer = None\n",
        "    for sp in [\"train\",\"val\",\"test\"]:\n",
        "        if n in split_names[sp]:\n",
        "            prefer = sp\n",
        "            break\n",
        "    src = find_label_in_raw(n, prefer_split=prefer)\n",
        "    if src:\n",
        "        shutil.copy2(src, DST_ROOT/\"label\"/n)\n",
        "        recovered.append({\"name\": n, \"from\": str(src)})\n",
        "    else:\n",
        "        not_found.append(n)\n",
        "\n",
        "# recompute label set after recovery\n",
        "L2 = {p.name for p in (DST_ROOT/\"label\").glob(\"*.png\")}\n",
        "triplets = A & B & L2\n",
        "\n",
        "# --- rewrite split lists to keep only valid triplets ---\n",
        "rewritten_counts = {}\n",
        "for sp in [\"train\",\"val\",\"test\"]:\n",
        "    names = split_names[sp]\n",
        "    if not names:\n",
        "        # if no existing list, just leave it empty (we won't guess the split)\n",
        "        rewritten = []\n",
        "    else:\n",
        "        rewritten = [n for n in names if n in triplets]\n",
        "    split_files[sp].parent.mkdir(parents=True, exist_ok=True)\n",
        "    split_files[sp].write_text(\"\\n\".join(rewritten))\n",
        "    rewritten_counts[sp] = len(rewritten)\n",
        "\n",
        "# --- save a small report ---\n",
        "report = {\n",
        "    \"A\": len(A), \"B\": len(B), \"label_before\": len(L), \"label_after\": len(L2),\n",
        "    \"orphans_checked\": len(orphans_ab),\n",
        "    \"recovered\": len(recovered),\n",
        "    \"not_found\": len(not_found),\n",
        "    \"split_counts\": rewritten_counts,\n",
        "}\n",
        "(DST_ROOT/\"mismatch_report.json\").write_text(json.dumps(\n",
        "    {\"report\": report, \"recovered\": recovered, \"not_found\": not_found}, indent=2))\n",
        "\n",
        "print(\"=== Sync summary ===\")\n",
        "print(report)\n",
        "print(\"Report saved to:\", DST_ROOT/\"mismatch_report.json\")\n"
      ],
      "metadata": {
        "id": "5hvfSq_R1pcb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f071ce6-d5f3-4775-a626-44142ac27c79"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Sync summary ===\n",
            "{'A': 5000, 'B': 5000, 'label_before': 4000, 'label_after': 5000, 'orphans_checked': 1000, 'recovered': 1000, 'not_found': 0, 'split_counts': {'train': 3500, 'val': 500, 'test': 1000}}\n",
            "Report saved to: /content/drive/MyDrive/datasets/S2Looking_MobileCDNet/mismatch_report.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path, re\n",
        "tf = Path(\"/content/Mobile-CDNet/Transforms.py\")\n",
        "src = tf.read_text()\n",
        "\n",
        "# Fix deprecated NumPy aliases everywhere\n",
        "src = src.replace(\"np.int)\",   \"np.int64)\")\n",
        "src = src.replace(\"np.int)\",   \"np.int64)\")\n",
        "src = src.replace(\"np.float)\", \"np.float32)\")\n",
        "src = src.replace(\"np.bool)\",  \"np.bool_)\")\n",
        "\n",
        "# Also handle cases with spaces like 'dtype = np.int' if present\n",
        "src = re.sub(r\"dtype\\s*=\\s*np\\.int\\b\",   \"dtype=np.int64\", src)\n",
        "src = re.sub(r\"dtype\\s*=\\s*np\\.float\\b\", \"dtype=np.float32\", src)\n",
        "src = re.sub(r\"dtype\\s*=\\s*np\\.bool\\b\",  \"dtype=np.bool_\", src)\n",
        "\n",
        "tf.write_text(src)\n",
        "print(\"✅ Patched deprecated NumPy dtypes in Transforms.py\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0q_5QuJmeV-",
        "outputId": "0719c07e-635f-4a44-d4ce-c3f409281342"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Patched deprecated NumPy dtypes in Transforms.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Mobile-CDNet\n",
        "!PYTHONPATH=/content/Mobile-CDNet python -m tools.train \\\n",
        "  --file_root \"/content/drive/MyDrive/datasets/S2Looking_MobileCDNet\" \\\n",
        "  --inWidth 256 --inHeight 256 \\\n",
        "  --batch_size 8 --num_workers 14 \\\n",
        "  --lr 0.001 --max_steps 5000 --step_loss 100 \\\n",
        "  --savedir \"/content/drive/MyDrive/experiments/mobilecdnet_s2looking\" \\\n",
        "  --resume False --onGPU True\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2iaM23ZeoDk",
        "outputId": "4b40a8b8-2f60-4f8b-969a-618766b38e37"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Mobile-CDNet\n",
            "Called with args:\n",
            "Namespace(file_root='/content/drive/MyDrive/datasets/S2Looking_MobileCDNet', inWidth=256, inHeight=256, max_steps=5000, num_workers=14, batch_size=8, step_loss=100, lr=0.001, lr_mode='poly', savedir='/content/drive/MyDrive/experiments/mobilecdnet_s2looking', resume='False', logFile='trainValLog.txt', onGPU=True, weight='', ms=0)\n",
            "loading imagenet pretrained mobilenetv2\n",
            "loaded imagenet pretrained mobilenetv2\n",
            "Total network parameters (excluding idr): 2946089\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 14 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "For each epoch, we have 437 batches\n",
            "=> no checkpoint found at '/content/drive/MyDrive/experiments/mobilecdnet_s2looking/S2Looking_MobileCDNet_iter_5000_lr_0.001/checkpoint.pth.tar'\n",
            "iteration: [872/5244] f1: 0.428 lr: 0.0008490 loss: 0.680 time:0.102 h63\n",
            "[60/63] F1: 0.189775 loss: 0.956 time: 0.023Epoch 1: Details\n",
            "\n",
            "Epoch No. 1:\tTrain Loss = 0.7248\tVal Loss = 0.9510\t F1(tr) = 0.3766\t F1(val) = 0.1538\n",
            "iteration: [1309/5244] f1: 0.520 lr: 0.0007722 loss: 0.551 time:0.089 h63\n",
            "[60/63] F1: 0.129070 loss: 1.041 time: 0.019Epoch 2: Details\n",
            "\n",
            "Epoch No. 2:\tTrain Loss = 0.6840\tVal Loss = 0.9998\t F1(tr) = 0.4115\t F1(val) = 0.1295\n",
            "iteration: [1746/5244] f1: 0.431 lr: 0.0006946 loss: 0.632 time:0.081 h63\n",
            "[60/63] F1: 0.275008 loss: 0.856 time: 0.033Epoch 3: Details\n",
            "\n",
            "Epoch No. 3:\tTrain Loss = 0.6619\tVal Loss = 0.8699\t F1(tr) = 0.4303\t F1(val) = 0.2522\n",
            "iteration: [2183/5244] f1: 0.419 lr: 0.0006160 loss: 0.633 time:0.070 h63\n",
            "[60/63] F1: 0.141421 loss: 1.018 time: 0.019Epoch 4: Details\n",
            "\n",
            "Epoch No. 4:\tTrain Loss = 0.6475\tVal Loss = 0.9998\t F1(tr) = 0.4484\t F1(val) = 0.1233\n",
            "iteration: [2620/5244] f1: 0.413 lr: 0.0005363 loss: 0.653 time:0.060 h63\n",
            "[60/63] F1: 0.097129 loss: 1.068 time: 0.024Epoch 5: Details\n",
            "\n",
            "Epoch No. 5:\tTrain Loss = 0.6239\tVal Loss = 1.0554\t F1(tr) = 0.4698\t F1(val) = 0.0616\n",
            "iteration: [3057/5244] f1: 0.479 lr: 0.0004552 loss: 0.590 time:0.050 h63\n",
            "[60/63] F1: 0.114223 loss: 1.262 time: 0.026Epoch 6: Details\n",
            "\n",
            "Epoch No. 6:\tTrain Loss = 0.6153\tVal Loss = 0.9185\t F1(tr) = 0.4803\t F1(val) = 0.2184\n",
            "iteration: [3494/5244] f1: 0.556 lr: 0.0003724 loss: 0.510 time:0.051 h63\n",
            "[60/63] F1: 0.090677 loss: 1.067 time: 0.019Epoch 7: Details\n",
            "\n",
            "Epoch No. 7:\tTrain Loss = 0.5948\tVal Loss = 1.0242\t F1(tr) = 0.4974\t F1(val) = 0.0925\n",
            "iteration: [3931/5244] f1: 0.561 lr: 0.0002876 loss: 0.493 time:0.070 h63\n",
            "[60/63] F1: 0.036606 loss: 1.383 time: 0.031Epoch 8: Details\n",
            "\n",
            "Epoch No. 8:\tTrain Loss = 0.5897\tVal Loss = 0.9796\t F1(tr) = 0.5038\t F1(val) = 0.1493\n",
            "iteration: [4368/5244] f1: 0.369 lr: 0.0001998 loss: 0.708 time:0.027 h63\n",
            "[60/63] F1: 0.392370 loss: 0.712 time: 0.031Epoch 9: Details\n",
            "\n",
            "Epoch No. 9:\tTrain Loss = 0.5766\tVal Loss = 0.7966\t F1(tr) = 0.5156\t F1(val) = 0.3097\n",
            "iteration: [4805/5244] f1: 0.556 lr: 0.0001073 loss: 0.515 time:0.010 h63\n",
            "[60/63] F1: 0.186835 loss: 0.980 time: 0.023Epoch 10: Details\n",
            "\n",
            "Epoch No. 10:\tTrain Loss = 0.5704\tVal Loss = 0.8487\t F1(tr) = 0.5241\t F1(val) = 0.2502\n",
            "iteration: [5242/5244] f1: 0.373 lr: 0.0000008 loss: 0.699 time:0.000 h63\n",
            "[60/63] F1: 0.525868 loss: 0.584 time: 0.018Epoch 11: Details\n",
            "\n",
            "Epoch No. 11:\tTrain Loss = 0.5548\tVal Loss = 0.5490\t F1(tr) = 0.5374\t F1(val) = 0.5627\n",
            "125\n",
            "[120/125] F1: 0.582127 loss: 0.505 time: 0.018\n",
            "Test :\t Kappa (te) = 0.5021\t IoU (te) = 0.3406\t F1 (te) = 0.5081\t R (te) = 0.5104\t P (te) = 0.5059\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, cv2, torch, numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# ===== Your exact paths (no slash) =====\n",
        "BEST = \"/content/drive/MyDrive/experiments/mobilecdnet_s2looking/S2Looking_MobileCDNet_iter_5000_lr_0.001best_model.pth\"\n",
        "CKPT = \"/content/drive/MyDrive/experiments/mobilecdnet_s2looking/S2Looking_MobileCDNet_iter_5000_lr_0.001checkpoint.pth.tar\"\n",
        "\n",
        "DATA_ROOT = \"/content/drive/MyDrive/datasets/S2Looking_MobileCDNet\"\n",
        "INW, INH = 256, 256\n",
        "N_SAMPLES = 10\n",
        "\n",
        "# Repo imports\n",
        "import sys\n",
        "sys.path.insert(0, \"/content/Mobile-CDNet\")\n",
        "from models.model import BaseNet\n",
        "import dataset as myDataLoader\n",
        "import Transforms as myTransforms\n",
        "\n",
        "# Pick which file exists\n",
        "ckpt_path = None\n",
        "ckpt_type = None\n",
        "if Path(BEST).exists():\n",
        "    ckpt_path, ckpt_type = BEST, \"state_dict\"\n",
        "elif Path(CKPT).exists():\n",
        "    ckpt_path, ckpt_type = CKPT, \"checkpoint\"\n",
        "else:\n",
        "    raise FileNotFoundError(\"Neither best_model nor checkpoint found at the given paths.\")\n",
        "\n",
        "print(\"Loading:\", ckpt_path, f\"({ckpt_type})\")\n",
        "\n",
        "# Dataset/Transforms\n",
        "mean = [0.406, 0.456, 0.485, 0.406, 0.456, 0.485]\n",
        "std  = [0.225, 0.224, 0.229, 0.225, 0.224, 0.229]\n",
        "valT = myTransforms.Compose([\n",
        "    myTransforms.Normalize(mean=mean, std=std),\n",
        "    myTransforms.Scale(INW, INH),\n",
        "    myTransforms.ToTensor()\n",
        "])\n",
        "\n",
        "val_ds = myDataLoader.Dataset(\"val\", file_root=DATA_ROOT, transform=valT)\n",
        "print(\"Val size:\", len(val_ds))\n",
        "\n",
        "# Model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = BaseNet(3,1)\n",
        "state = torch.load(ckpt_path, map_location=\"cpu\")\n",
        "if ckpt_type == \"checkpoint\":\n",
        "    state = state[\"state_dict\"]\n",
        "model.load_state_dict(state)\n",
        "model.eval().to(device)\n",
        "\n",
        "# Output folder (next to your checkpoint file)\n",
        "out_dir = Path(ckpt_path).parent / (Path(ckpt_path).name + \"_val_preds\")\n",
        "out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Save a handful of predictions\n",
        "step = max(1, len(val_ds)//N_SAMPLES) if len(val_ds)>N_SAMPLES else 1\n",
        "indices = list(range(0, min(len(val_ds), N_SAMPLES*step), step))\n",
        "\n",
        "for i in indices:\n",
        "    img, gt = val_ds[i]          # img: 6xHxW, gt: 1xHxW\n",
        "    pre  = img[:3].unsqueeze(0).to(device).float()\n",
        "    post = img[3:6].unsqueeze(0).to(device).float()\n",
        "    with torch.no_grad():\n",
        "        prob = model(pre, post)           # Bx1xHxW\n",
        "        pred = (prob > 0.5).float()\n",
        "    m = (pred[0,0].cpu().numpy()*255).astype(np.uint8)\n",
        "    cv2.imwrite(str(out_dir / f\"val_{i:05d}.png\"), m)\n",
        "\n",
        "print(\"Saved predictions to:\", out_dir)\n"
      ],
      "metadata": {
        "id": "0uB6sYlPMKf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Mobile-CDNet\n",
        "!PYTHONPATH=/content/Mobile-CDNet python -m tools.train \\\n",
        "  --file_root \"/content/drive/MyDrive/datasets/S2Looking_MobileCDNet\" \\\n",
        "  --inWidth 256 --inHeight 256 \\\n",
        "  --batch_size 8 --num_workers 14 \\\n",
        "  --lr 0.001 --max_steps 10000 --step_loss 100 \\\n",
        "  --savedir \"/content/drive/MyDrive/experiments/mobilecdnet_s2looking\" \\\n",
        "  --resume True --onGPU True\n"
      ],
      "metadata": {
        "id": "xsvYA87FL2Tw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qxnd8DGASQu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S97CSYu6SQHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X1fr5pP8SPeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i3WT2nMoSO9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RhlMh0ZxSNbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e0w2qmj4b5Zg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make a persistent folder inside your Google Drive\n",
        "!mkdir -p \"/content/drive/MyDrive/dev\"\n",
        "\n",
        "# copy your working repo from Colab's temp storage to Drive\n",
        "!rsync -a --delete \"/content/Mobile-CDNet/\" \"/content/drive/MyDrive/dev/Mobile-CDNet/\"\n",
        "\n"
      ],
      "metadata": {
        "id": "ulvC-9oFKJn0"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p \"/content/drive/MyDrive/dev\"\n",
        "!rsync -a --delete \"/content/Mobile-CDNet/\" \"/content/drive/MyDrive/dev/Mobile-CDNet/\"\n",
        "%cd /content/drive/MyDrive/dev/Mobile-CDNet\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzqnUEuOKUaY",
        "outputId": "d02b1328-8257-4fa8-c4bb-390cbe0108f7"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/dev/Mobile-CDNet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p \"/content/drive/MyDrive/dev\"\n",
        "!rsync -a --delete \"/content/Mobile-CDNet/\" \"/content/drive/MyDrive/dev/Mobile-CDNet/\"\n",
        "%cd /content/drive/MyDrive/dev/Mobile-CDNet\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AC4m-YLKlo6",
        "outputId": "fec1450d-bf8e-4d64-9090-ec556bd61f28"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/dev/Mobile-CDNet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p \"/content/drive/MyDrive/experiments/mobilecdnet_s2looking\"\n",
        "# If you already have a best model path, also save a 'last.pt' alias:\n",
        "!cp -n \"/content/drive/MyDrive/experiments/mobilecdnet_s2looking/S2Looking_MobileCDNet_iter_5000_lr_0.001best_model.pth\" \\\n",
        "       \"/content/drive/MyDrive/experiments/mobilecdnet_s2looking/last.pt\"\n",
        "!ls -lh \"/content/drive/MyDrive/experiments/mobilecdnet_s2looking\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsOA3J2AMUcL",
        "outputId": "95d03b69-690a-4fe6-9d3a-51d411a350ea"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 55M\n",
            "-rw------- 1 root root  12M Oct  9 18:17 last.pt\n",
            "drwx------ 4 root root 4.0K Oct  9 18:16 S2Looking_MobileCDNet_iter_5000_lr_0.001\n",
            "-rw------- 1 root root  12M Oct  9 17:54 S2Looking_MobileCDNet_iter_5000_lr_0.001best_model.pth\n",
            "-rw------- 1 root root  32M Oct  9 17:54 S2Looking_MobileCDNet_iter_5000_lr_0.001checkpoint.pth.tar\n",
            "-rw------- 1 root root  581 Oct  9 17:58 S2Looking_MobileCDNet_iter_5000_lr_0.001trainValLog.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/dev/Mobile-CDNet\n",
        "!pip freeze > requirements.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QZqrHCDKoIZ",
        "outputId": "c48da81e-7535-4c58-bfc0-cb7626ee09a1"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/dev/Mobile-CDNet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/dev/Mobile-CDNet\n",
        "!git init\n",
        "!git config user.name \"karaxstone7\"\n",
        "!git config user.email \"karaxstone7@gmail.com\"\n",
        "!git add -A\n",
        "!git commit -m \"Colab snapshot: Mobile-CDNet + S2Looking pretraining\"\n",
        "!git branch -M main\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ag0lYtXWMWdU",
        "outputId": "54521895-658b-4e75-f065-1ca66ac0f0ff"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/dev/Mobile-CDNet\n",
            "Reinitialized existing Git repository in /content/drive/MyDrive/dev/Mobile-CDNet/.git/\n",
            "[main b879e27] Colab snapshot: Mobile-CDNet + S2Looking pretraining\n",
            " 12 files changed, 839 insertions(+), 135 deletions(-)\n",
            " create mode 100644 __pycache__/Transforms.cpython-312.pyc\n",
            " create mode 100644 __pycache__/dataset.cpython-312.pyc\n",
            " create mode 100644 __pycache__/metric_tool.cpython-312.pyc\n",
            " create mode 100644 __pycache__/utils.cpython-312.pyc\n",
            " rewrite dataset.py (77%)\n",
            " create mode 100644 models/__pycache__/MobileNetV2.cpython-312.pyc\n",
            " create mode 100644 models/__pycache__/__init__.cpython-312.pyc\n",
            " create mode 100644 models/__pycache__/model.cpython-312.pyc\n",
            " create mode 100644 requirements.txt\n",
            " create mode 100644 tools/__pycache__/train.cpython-312.pyc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git remote remove origin >/dev/null 2>&1 || true\n",
        "!git remote add origin https://github.com/karaxstone7/mobilecdnet-ps10.git\n"
      ],
      "metadata": {
        "id": "FET_VDywMY2a"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git config credential.helper store\n"
      ],
      "metadata": {
        "id": "j5DWqWVcUYvy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}